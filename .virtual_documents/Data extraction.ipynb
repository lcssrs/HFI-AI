from bs4 import BeautifulSoup
from urllib.request import urlopen

import pandas as pd





#Scrapping dates from ECB's website

url = "https://www.ecb.europa.eu/pub/projections/html/all-releases.en.html"
page = urlopen(url)
html = page.read().decode("utf-8")
soup = BeautifulSoup(html, "html.parser")

#Creating dataframe
dat = soup.find_all("dt")
dats=[0 for i in range((len(dat)))]
for i in range(0,len(dat)): 
    dats[i]=dat[i]["isodate"]

#removing duplicates, noisy data and reseting index
dates = pd.DataFrame(dats, columns=["Dates"]).drop_duplicates()
dates = dates.drop(labels=[6,11,16,21,26,31,36,38,41,46,51,56,61,64,67], axis=0).reset_index()
dates = dates.drop('index', axis=1)


url = "https://www.ecb.europa.eu/press/govcdec/mopo/html/index.en.html"
response = requests.get(url)
html = response.text

#Creating Beautiful Soup object
soup = BeautifulSoup(html, 'html.parser')
# Buscando todos os elementos 'a', que representam links
links = soup.find_all('a')

# Extraindo e imprimindo os atributos 'href' de cada link
for link in links:
    href = link.get('href')
    if href:
        print(href)



links


for titulo in dat:
    print(titulo.text.strip())



#Press release
https://www.ecb.europa.eu/press/govcdec/mopo/html/index.en.html

#Press conference
https://www.ecb.europa.eu/press/press_conference/monetary-policy-statement/html/index.en.html



