{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb721fba-9d03-4aed-9cfd-22f63eb73155",
   "metadata": {},
   "source": [
    "# Encoding text data\n",
    "\n",
    "Data in unstructured form is free form data such as text. We have to transfor it in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5b40e6-050a-4fa8-8f71-daaf7df271ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'inaugural_speeches.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m speech_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minaugural_speeches.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inaugural_speeches.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "speech_df = pd.read_csv('inaugural_speeches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be6dbde-4f0b-4acf-a045-88032c2be25b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speech_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m speech_df\u001b[38;5;241m.\u001b[39mdtypes\n",
      "\u001b[1;31mNameError\u001b[0m: name 'speech_df' is not defined"
     ]
    }
   ],
   "source": [
    "speech_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1661808-8994-4bbd-b6f9-300cf50e3692",
   "metadata": {},
   "source": [
    "Text is saved in a particular column of the dataframe. Also, this text will need to be cleaned from characters such as punctuation then we can create features after that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23d8ed00-2fe4-44d8-99d2-8bd11caeb288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    fellow citizens of the senate and of the house...\n",
      "1    fellow citizens   i am again called upon by th...\n",
      "2    when it was first perceived  in early times  t...\n",
      "3    friends and fellow citizens   called upon to u...\n",
      "4    proceeding  fellow citizens  to that qualifica...\n",
      "Name: text_clean, dtype: object\n",
      "                                           text_clean  char_cnt  word_cnt  \\\n",
      "0   fellow citizens of the senate and of the house...      8616      1432   \n",
      "1   fellow citizens   i am again called upon by th...       787       135   \n",
      "2   when it was first perceived  in early times  t...     13871      2323   \n",
      "3   friends and fellow citizens   called upon to u...     10144      1736   \n",
      "4   proceeding  fellow citizens  to that qualifica...     12902      2169   \n",
      "5   unwilling to depart from examples of the most ...      7003      1179   \n",
      "6   about to add the solemnity of an oath to the o...      7148      1211   \n",
      "7   i should be destitute of feeling if i was not ...     19894      3382   \n",
      "8   fellow citizens   i shall not attempt to descr...     26322      4466   \n",
      "9   in compliance with an usage coeval with the ex...     17753      2922   \n",
      "10  fellow citizens   about to undertake the arduo...      6818      1130   \n",
      "11  fellow citizens   the will of the american peo...      7061      1179   \n",
      "12  fellow citizens  the practice of all my predec...     23527      3912   \n",
      "13  called from a retirement which i had supposed ...     32706      5585   \n",
      "14  fellow citizens   without solicitation on my p...     28739      4821   \n",
      "15  elected by the american people to the highest ...      6599      1092   \n",
      "16  my countrymen   it a relief to feel that no he...     20089      3348   \n",
      "17  fellow citizens   i appear before you this day...     16820      2839   \n",
      "18  fellow citizens of the united states   in comp...     21032      3642   \n",
      "19  fellow countrymen     at this second appearing...      3934       706   \n",
      "20  citizens of the united states   your suffrages...      6521      1138   \n",
      "21  fellow citizens   under providence i have been...      7736      1342   \n",
      "22  fellow citizens   we have assembled to repeat ...     14969      2498   \n",
      "23  fellow citizens   we stand to day upon an emin...     17774      2990   \n",
      "24  fellow citizens   in the presence of this vast...     10155      1695   \n",
      "25  fellow citizens   there is no constitutional o...     26175      4399   \n",
      "26  my fellow citizens   in obedience of the manda...     12340      2028   \n",
      "27  fellow citizens   in obedience to the will of ...     23691      3980   \n",
      "28  my fellow citizens   when we assembled here on...     13426      2216   \n",
      "29  my fellow citizens  no people on earth have mo...      5565       991   \n",
      "30  my fellow citizens   anyone who has taken the ...     32160      5439   \n",
      "31  there has been a change of government  it bega...      9554      1712   \n",
      "32  my fellow citizens   the four years which have...      8402      1535   \n",
      "33  my countrymen   when one surveys the world abo...     20294      3348   \n",
      "34  my countrymen   no one can contemplate current...     23937      4055   \n",
      "35  my countrymen   this occasion is not alone the...     22961      3771   \n",
      "36  i am certain that my fellow americans expect t...     10910      1888   \n",
      "37  when four years ago we met to inaugurate a pre...     10629      1831   \n",
      "38  on each national day of inauguration since    ...      7674      1371   \n",
      "39  mr  chief justice  mr  vice president  my frie...      3086       573   \n",
      "40  mr  vice president  mr  chief justice  and fel...     13707      2292   \n",
      "41  my friends  before i begin the expression of t...     14003      2475   \n",
      "42  the price of peace mr  chairman  mr  vice pres...      9277      1688   \n",
      "43  vice president johnson  mr  speaker  mr  chief...      7706      1390   \n",
      "44  my fellow countrymen  on this occasion  the oa...      8242      1502   \n",
      "45  senator dirksen  mr  chief justice  mr  vice p...     11701      2152   \n",
      "46  mr  vice president  mr  speaker  mr  chief jus...     10048      1835   \n",
      "47  for myself and for our nation  i want to thank...      6934      1238   \n",
      "48  senator hatfield  mr  chief justice  mr  presi...     13787      2457   \n",
      "49  senator mathias  chief justice burger  vice pr...     14601      2586   \n",
      "50  mr  chief justice  mr  president  vice preside...     12536      2342   \n",
      "51  my fellow citizens today we celebrate the myst...      9119      1608   \n",
      "52  my fellow citizens at this last presidential i...     12374      2201   \n",
      "53  president clinton  distinguished guests and my...      9084      1606   \n",
      "54  vice president cheney  mr  chief justice  pres...     12199      2122   \n",
      "55  my fellow citizens     i stand here today humb...     13637      2452   \n",
      "56  vice president biden  mr  chief justice  membe...     12174      2151   \n",
      "57  chief justice roberts  president carter  presi...      8555      1488   \n",
      "\n",
      "    avg_word_length  \n",
      "0          6.016760  \n",
      "1          5.829630  \n",
      "2          5.971158  \n",
      "3          5.843318  \n",
      "4          5.948363  \n",
      "5          5.939779  \n",
      "6          5.902560  \n",
      "7          5.882318  \n",
      "8          5.893865  \n",
      "9          6.075633  \n",
      "10         6.033628  \n",
      "11         5.988974  \n",
      "12         6.014059  \n",
      "13         5.856043  \n",
      "14         5.961211  \n",
      "15         6.043040  \n",
      "16         6.000299  \n",
      "17         5.924621  \n",
      "18         5.774849  \n",
      "19         5.572238  \n",
      "20         5.730228  \n",
      "21         5.764531  \n",
      "22         5.992394  \n",
      "23         5.944482  \n",
      "24         5.991150  \n",
      "25         5.950216  \n",
      "26         6.084813  \n",
      "27         5.952513  \n",
      "28         6.058664  \n",
      "29         5.615540  \n",
      "30         5.912852  \n",
      "31         5.580607  \n",
      "32         5.473616  \n",
      "33         6.061529  \n",
      "34         5.903083  \n",
      "35         6.088836  \n",
      "36         5.778602  \n",
      "37         5.805025  \n",
      "38         5.597374  \n",
      "39         5.385689  \n",
      "40         5.980366  \n",
      "41         5.657778  \n",
      "42         5.495853  \n",
      "43         5.543885  \n",
      "44         5.487350  \n",
      "45         5.437268  \n",
      "46         5.475749  \n",
      "47         5.600969  \n",
      "48         5.611315  \n",
      "49         5.646172  \n",
      "50         5.352690  \n",
      "51         5.671020  \n",
      "52         5.621990  \n",
      "53         5.656289  \n",
      "54         5.748822  \n",
      "55         5.561582  \n",
      "56         5.659693  \n",
      "57         5.749328  \n"
     ]
    }
   ],
   "source": [
    "#Removing unwanted characters\n",
    "# Replace all non letter characters with a whitespace\n",
    "speech_df['text_clean'] = speech_df['text'].str.replace(r'[^a-zA-Z]', ' ', regex=True)\n",
    "\n",
    "# Change to lower case\n",
    "speech_df['text_clean'] = speech_df['text_clean'].str.lower()\n",
    "\n",
    "# Print the first 5 rows of the text_clean column\n",
    "print(speech_df['text_clean'].head())\n",
    "\n",
    "# Find the length of each text\n",
    "speech_df['char_cnt'] = speech_df['text_clean'].str.len()\n",
    "\n",
    "# Count the number of words in each text\n",
    "speech_df['word_cnt'] = speech_df['text_clean'].str.split().str.len()\n",
    "\n",
    "# Find the average length of word\n",
    "speech_df['avg_word_length'] = speech_df['char_cnt'] / speech_df['word_cnt']\n",
    "\n",
    "# Print the first 5 rows of these columns\n",
    "print(speech_df[['text_clean', 'char_cnt', 'word_cnt', 'avg_word_length']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1dd71e-d4cd-4f83-99db-e3e7500bddd5",
   "metadata": {},
   "source": [
    "We basically create a column for each word and count the number of times it appears, to avoid creating a too large vector. For that reason, we use the min_df and max_df arguments. They set the minimum and maximum number of documents in which the word can occur. Basically excludes rare and very common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f171524c-208d-4ab0-a752-bccae0561567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandon' 'abandoned' 'abandonment' ... 'zealous' 'zealously' 'zone']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(58, 818)\n",
      "                Name         Inaugural Address                      Date  \\\n",
      "0  George Washington   First Inaugural Address  Thursday, April 30, 1789   \n",
      "1  George Washington  Second Inaugural Address     Monday, March 4, 1793   \n",
      "2         John Adams         Inaugural Address   Saturday, March 4, 1797   \n",
      "3   Thomas Jefferson   First Inaugural Address  Wednesday, March 4, 1801   \n",
      "4   Thomas Jefferson  Second Inaugural Address     Monday, March 4, 1805   \n",
      "\n",
      "                                                text  \\\n",
      "0  Fellow-Citizens of the Senate and of the House...   \n",
      "1  Fellow Citizens:  I AM again called upon by th...   \n",
      "2  WHEN it was first perceived, in early times, t...   \n",
      "3  Friends and Fellow-Citizens:  CALLED upon to u...   \n",
      "4  PROCEEDING, fellow-citizens, to that qualifica...   \n",
      "\n",
      "                                          text_clean  char_cnt  word_cnt  \\\n",
      "0  fellow citizens of the senate and of the house...      8616      1432   \n",
      "1  fellow citizens   i am again called upon by th...       787       135   \n",
      "2  when it was first perceived  in early times  t...     13871      2323   \n",
      "3  friends and fellow citizens   called upon to u...     10144      1736   \n",
      "4  proceeding  fellow citizens  to that qualifica...     12902      2169   \n",
      "\n",
      "   avg_word_length  Counts_abiding  Counts_ability  ...  Counts_women  \\\n",
      "0         6.016760               0               0  ...             0   \n",
      "1         5.829630               0               0  ...             0   \n",
      "2         5.971158               0               0  ...             0   \n",
      "3         5.843318               0               0  ...             0   \n",
      "4         5.948363               0               0  ...             0   \n",
      "\n",
      "   Counts_words  Counts_work  Counts_wrong  Counts_year  Counts_years  \\\n",
      "0             0            0             0            0             1   \n",
      "1             0            0             0            0             0   \n",
      "2             0            0             0            2             3   \n",
      "3             0            1             2            0             0   \n",
      "4             0            0             0            2             2   \n",
      "\n",
      "   Counts_yet  Counts_you  Counts_young  Counts_your  \n",
      "0           0           5             0            9  \n",
      "1           0           0             0            1  \n",
      "2           0           0             0            1  \n",
      "3           2           7             0            7  \n",
      "4           2           4             0            4  \n",
      "\n",
      "[5 rows x 826 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "# Fit the vectorizer\n",
    "cv.fit(speech_df['text_clean'])\n",
    "\n",
    "# Print feature names\n",
    "print(cv.get_feature_names_out())\n",
    "\n",
    "# Apply the vectorizer\n",
    "cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "\n",
    "#trasndformed -> sparse array with a row for every text and column for every word counted\n",
    "\n",
    "\n",
    "\n",
    "# Print the full array\n",
    "cv_array = cv_transformed.toarray()\n",
    "print(cv_array)\n",
    "\n",
    "\n",
    "##### AGAIN WITH A DIFFERENT VERSION\n",
    "# Specify arguements to limit the number of features generated\n",
    "cv = CountVectorizer(min_df=.2, max_df=.8)\n",
    "\n",
    "# Fit, transform, and convert into array\n",
    "cv_transformed = cv.fit_transform(speech_df['text_clean'])\n",
    "cv_array = cv_transformed.toarray()\n",
    "\n",
    "# Print the array shape\n",
    "print(cv_array.shape)\n",
    "######################################\n",
    "\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "cv_df = pd.DataFrame(cv_array, \n",
    "                     columns=cv.get_feature_names_out()).add_prefix('Counts_')\n",
    "\n",
    "# Add the new columns to the original DataFrame\n",
    "speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)\n",
    "print(speech_df_new.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecdf9bb-65b5-4db2-b956-1a08ec7d7410",
   "metadata": {},
   "source": [
    "# TD-IDF Representation\n",
    "\n",
    "$$ TF-IDF = \\frac{\\frac{Count \\ of \\ word \\ occurances}{Total \\ words \\ in \\ document}}{\\log(\\frac{Number \\ of \\ docs \\ word \\ is \\ in}{Total \\ number \\ of \\ docs})}  $$\n",
    "\n",
    "Decreases the value of common words while increasing the value of words that do not occur in many documents. The idea is to weight a word the more it appears in the particular document but to deleverage if it appears in too many documents\n",
    "\n",
    "After transforming the data always check if the words valuation make sense by examining a row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d07137c5-8018-4a12-8db8-030e1d1b1eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TFIDF_action  TFIDF_administration  TFIDF_america  TFIDF_american  \\\n",
      "0      0.000000              0.133415       0.000000        0.105388   \n",
      "1      0.000000              0.261016       0.266097        0.000000   \n",
      "2      0.000000              0.092436       0.157058        0.073018   \n",
      "3      0.000000              0.092693       0.000000        0.000000   \n",
      "4      0.041334              0.039761       0.000000        0.031408   \n",
      "\n",
      "   TFIDF_americans  TFIDF_believe  TFIDF_best  TFIDF_better  TFIDF_change  \\\n",
      "0              0.0       0.000000    0.000000      0.000000      0.000000   \n",
      "1              0.0       0.000000    0.000000      0.000000      0.000000   \n",
      "2              0.0       0.000000    0.026112      0.060460      0.000000   \n",
      "3              0.0       0.090942    0.117831      0.045471      0.053335   \n",
      "4              0.0       0.000000    0.067393      0.039011      0.091514   \n",
      "\n",
      "   TFIDF_citizens  ...  TFIDF_things  TFIDF_time  TFIDF_today  TFIDF_union  \\\n",
      "0        0.229644  ...      0.000000    0.045929          0.0     0.136012   \n",
      "1        0.179712  ...      0.000000    0.000000          0.0     0.000000   \n",
      "2        0.106072  ...      0.032030    0.021214          0.0     0.062823   \n",
      "3        0.223369  ...      0.048179    0.000000          0.0     0.094497   \n",
      "4        0.273760  ...      0.082667    0.164256          0.0     0.121605   \n",
      "\n",
      "   TFIDF_united  TFIDF_war  TFIDF_way  TFIDF_work  TFIDF_world  TFIDF_years  \n",
      "0      0.203593   0.000000   0.060755    0.000000     0.045929     0.052694  \n",
      "1      0.199157   0.000000   0.000000    0.000000     0.000000     0.000000  \n",
      "2      0.070529   0.024339   0.000000    0.000000     0.063643     0.073018  \n",
      "3      0.000000   0.036610   0.000000    0.039277     0.095729     0.000000  \n",
      "4      0.030338   0.094225   0.000000    0.000000     0.054752     0.062817  \n",
      "\n",
      "[5 rows x 100 columns]\n",
      "TFIDF_government    0.367430\n",
      "TFIDF_public        0.333237\n",
      "TFIDF_present       0.315182\n",
      "TFIDF_duty          0.238637\n",
      "TFIDF_country       0.229644\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit the vectroizer and transform the data\n",
    "tv_transformed = tv.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "tv_df = pd.DataFrame(tv_transformed.toarray(), \n",
    "                     columns=tv.get_feature_names_out()).add_prefix('TFIDF_')\n",
    "print(tv_df.head())\n",
    "\n",
    "# Isolate the row to be examined\n",
    "sample_row = tv_df.iloc[0]\n",
    "\n",
    "# Print the top 5 words of the sorted output\n",
    "print(sample_row.sort_values(ascending=False).head())\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c213a2f-e5d9-4126-9b45-e2f1582ec11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#I need to create train test split to run below this line\n",
    "# Fit the vectroizer and transform the data\n",
    "tv_transformed = tv.fit_transform(train_speech_df['text_clean'])\n",
    "\n",
    "# Transform test data\n",
    "test_tv_transformed = tv.transform(test_speech_df['text_clean'])\n",
    "\n",
    "# Create new features for the test set\n",
    "test_tv_df = pd.DataFrame(test_tv_transformed.toarray(), \n",
    "                          columns=tv.get_feature_names_out()).add_prefix('TFIDF_')\n",
    "print(test_tv_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e65ccec-2571-4ca5-a917-5b059fa75731",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "N-grams take the idea that words have meaning in a context and therefore their order matters. ngram_range(min, max) gets clusters of words in sequence instead of single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0aee38a3-95c2-4bbc-acf7-5a4a1c631847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability preserve protect' 'agriculture commerce manufactures'\n",
      " 'america ideal freedom' 'amity mutual concession' 'anchor peace home'\n",
      " 'ask bow heads' 'best ability preserve' 'best interests country'\n",
      " 'bless god bless' 'bless united states' 'chief justice mr'\n",
      " 'children children children' 'citizens united states'\n",
      " 'civil religious liberty' 'civil service reform' 'commerce united states'\n",
      " 'confidence fellow citizens' 'congress extraordinary session'\n",
      " 'constitution does expressly' 'constitution united states'\n",
      " 'coordinate branches government' 'day task people'\n",
      " 'defend constitution united' 'distinction powers granted'\n",
      " 'distinguished guests fellow' 'does expressly say' 'equal exact justice'\n",
      " 'era good feeling' 'executive branch government'\n",
      " 'faithfully execute office' 'fellow citizens assembled'\n",
      " 'fellow citizens called' 'fellow citizens large' 'fellow citizens world'\n",
      " 'form perfect union' 'general welfare secure' 'god bless america'\n",
      " 'god bless god' 'good greatest number' 'government peace war'\n",
      " 'government united states' 'granted federal government'\n",
      " 'great body people' 'great political parties' 'greatest good greatest'\n",
      " 'guests fellow citizens' 'invasion wars powers' 'land new promise'\n",
      " 'laws faithfully executed' 'letter spirit constitution'\n",
      " 'liberty pursuit happiness' 'life liberty pursuit'\n",
      " 'local self government' 'make hard choices' 'men women children'\n",
      " 'mr chief justice' 'mr majority leader' 'mr president vice'\n",
      " 'mr speaker mr' 'mr vice president' 'nation like person'\n",
      " 'new breeze blowing' 'new states admitted' 'north south east'\n",
      " 'oath prescribed constitution' 'office president united'\n",
      " 'passed generation generation' 'peace shall strive'\n",
      " 'people united states' 'physical moral political' 'policy united states'\n",
      " 'power general government' 'preservation general government'\n",
      " 'preservation sacred liberty' 'preserve protect defend'\n",
      " 'president united states' 'president vice president'\n",
      " 'promote general welfare' 'proof confidence fellow'\n",
      " 'protect defend constitution' 'protection great interests'\n",
      " 'reform civil service' 'reserved states people'\n",
      " 'respect individual human' 'right self government'\n",
      " 'secure blessings liberty' 'south east west'\n",
      " 'sovereignty general government' 'states admitted union'\n",
      " 'territories united states' 'thank god bless' 'turning away old'\n",
      " 'united states america' 'united states best' 'united states government'\n",
      " 'united states great' 'united states maintain' 'united states territory'\n",
      " 'vice president mr' 'welfare secure blessings']\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate a trigram vectorizer\n",
    "cv_trigram_vec = CountVectorizer(max_features=100, \n",
    "                                 stop_words='english', \n",
    "                                 ngram_range =(3,3))\n",
    "\n",
    "# Fit and apply trigram vectorizer\n",
    "cv_trigram = cv_trigram_vec.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the trigram features\n",
    "print(cv_trigram_vec.get_feature_names_out())\n",
    "\n",
    "# Create a DataFrame of the features\n",
    "cv_tri_df = pd.DataFrame(cv_trigram.toarray(),\n",
    "                 columns=cv_trigram_vec.get_feature_names()).add_prefix('Counts_')\n",
    "\n",
    "# Print the top 5 words in the sorted output\n",
    "print(cv_tri_df.sum().sort_values(ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5effae19-ef4b-4962-84a1-4031ec8c6e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
